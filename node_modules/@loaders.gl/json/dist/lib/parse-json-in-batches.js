"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.rebuildJsonObject = void 0;
const schema_1 = require("@loaders.gl/schema");
const loader_utils_1 = require("@loaders.gl/loader-utils");
const streaming_json_parser_1 = __importDefault(require("./parser/streaming-json-parser"));
const jsonpath_1 = __importDefault(require("./jsonpath/jsonpath"));
// TODO - support batch size 0 = no batching/single batch?
// eslint-disable-next-line max-statements, complexity
async function* parseJSONInBatches(binaryAsyncIterator, options) {
    const asyncIterator = (0, loader_utils_1.makeTextDecoderIterator)(binaryAsyncIterator);
    const { metadata } = options;
    const { jsonpaths } = options.json || {};
    let isFirstChunk = true;
    // TODO fix Schema deduction
    const schema = null; // new Schema([]);
    const shape = options?.json?.shape || 'row-table';
    // @ts-ignore
    const tableBatchBuilder = new schema_1.TableBatchBuilder(schema, {
        ...options,
        shape
    });
    const parser = new streaming_json_parser_1.default({ jsonpaths });
    for await (const chunk of asyncIterator) {
        const rows = parser.write(chunk);
        const jsonpath = rows.length > 0 && parser.getStreamingJsonPathAsString();
        if (rows.length > 0 && isFirstChunk) {
            if (metadata) {
                const initialBatch = {
                    // Common fields
                    shape,
                    batchType: 'partial-result',
                    data: [],
                    length: 0,
                    bytesUsed: 0,
                    // JSON additions
                    container: parser.getPartialResult(),
                    jsonpath
                };
                yield initialBatch;
            }
            isFirstChunk = false;
            // schema = deduceSchema(rows);
        }
        // Add the row
        for (const row of rows) {
            tableBatchBuilder.addRow(row);
            // If a batch has been completed, emit it
            const batch = tableBatchBuilder.getFullBatch({ jsonpath });
            if (batch) {
                yield batch;
            }
        }
        tableBatchBuilder.chunkComplete(chunk);
        const batch = tableBatchBuilder.getFullBatch({ jsonpath });
        if (batch) {
            yield batch;
        }
    }
    // yield final batch
    const jsonpath = parser.getStreamingJsonPathAsString();
    const batch = tableBatchBuilder.getFinalBatch({ jsonpath });
    if (batch) {
        yield batch;
    }
    if (metadata) {
        const finalBatch = {
            shape,
            batchType: 'final-result',
            container: parser.getPartialResult(),
            jsonpath: parser.getStreamingJsonPathAsString(),
            data: [],
            length: 0
            // schema: null
        };
        yield finalBatch;
    }
}
exports.default = parseJSONInBatches;
function rebuildJsonObject(batch, data) {
    // Last batch will have this special type and will provide all the root object of the parsed file
    (0, loader_utils_1.assert)(batch.batchType === 'final-result');
    // The streamed JSON data is a top level array (jsonpath = '$'), just return the array of row objects
    if (batch.jsonpath === '$') {
        return data;
    }
    // (jsonpath !== '$') The streamed data is not a top level array, so stitch it back in to the top-level object
    if (batch.jsonpath && batch.jsonpath.length > 1) {
        const topLevelObject = batch.container;
        const streamingPath = new jsonpath_1.default(batch.jsonpath);
        streamingPath.setFieldAtPath(topLevelObject, data);
        return topLevelObject;
    }
    // No jsonpath, in this case nothing was streamed.
    return batch.container;
}
exports.rebuildJsonObject = rebuildJsonObject;
